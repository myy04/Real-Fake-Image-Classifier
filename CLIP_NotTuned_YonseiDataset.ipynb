{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPJ0qVAMPxW15g8ks75uQ6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myy04/Real-Fake-Image-Classifier/blob/main/CLIP_NotTuned_YonseiDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn matplotlib\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoProcessor, get_linear_schedule_with_warmup\n",
        "import os\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import Tensor\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ciplab/real-and-fake-face-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w3w1MhDLglU",
        "outputId": "22394aa9-dfaa-48e9-b509-8b4ec6b2346e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ciplab/real-and-fake-face-detection?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 431M/431M [00:24<00:00, 18.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ciplab/real-and-fake-face-detection/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "\n",
        "        real_images_directory = os.path.join(path, 'training_real')\n",
        "        fake_images_directory = os.path.join(path, 'training_fake')\n",
        "\n",
        "        real_images = []\n",
        "        fake_images = []\n",
        "\n",
        "        for file in os.listdir(real_images_directory):\n",
        "            real_images.append(os.path.join(real_images_directory, file))\n",
        "\n",
        "        for file in os.listdir(fake_images_directory):\n",
        "            fake_images.append(os.path.join(fake_images_directory, file))\n",
        "\n",
        "        self.images = []\n",
        "        for i in range(min(len(real_images), len(fake_images))):\n",
        "            self.images.append((real_images[i], torch.tensor([1, 0], dtype = torch.float32)))\n",
        "            self.images.append((fake_images[i], torch.tensor([0, 1], dtype = torch.float32)))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx][0]).convert(\"RGB\")\n",
        "        image = self.transform(image) # Apply transform if provided\n",
        "        return image, self.images[idx][1]\n",
        "\n",
        "\n",
        "eval_dataset = ImageDataset(os.path.join(path, 'real_and_fake_face'))\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "SjBDnLuCLmXQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVMWR99ID3z6",
        "outputId": "037bf2f8-d791-4822-88d2-2fec369d9050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation:\n",
            "Accuracy: 50.0\n",
            "Precision: 0.50\n",
            "Recall: 1.00\n",
            "F1 Score: 0.67\n"
          ]
        }
      ],
      "source": [
        "\n",
        "clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# --- Model Loading and Text Prompts ---\n",
        "clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "text = [\"a photo of a real human\", \"an AI generated photo of a human\"]\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "all_predicted_labels = []\n",
        "all_true_labels = []\n",
        "num_of_correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculations during evaluation\n",
        "\n",
        "    for images, labels_batch in eval_loader:\n",
        "        images, labels_batch = images.to(device), labels_batch.to(device)\n",
        "        for image, label in zip(images, labels_batch):\n",
        "\n",
        "            inputs = clip_processor(text=text, images=[image.cpu()], return_tensors=\"pt\", padding=True).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            logits_per_image = outputs.logits_per_image\n",
        "            probs = logits_per_image.softmax(dim=1)\n",
        "            predicted_class = probs.argmax(dim=1).item()\n",
        "            total += 1\n",
        "            num_of_correct += (predicted_class == label.argmax(dim=0).item())\n",
        "            all_predicted_labels.append(predicted_class)\n",
        "            all_true_labels.append(label.argmax(dim=0).item())\n",
        "\n",
        "# --- Calculate and Print Metrics ---\n",
        "precision = precision_score(all_true_labels, all_predicted_labels)\n",
        "recall = recall_score(all_true_labels, all_predicted_labels)\n",
        "f1 = f1_score(all_true_labels, all_predicted_labels)\n",
        "\n",
        "print(f\"Evaluation:\")\n",
        "print(f'Accuracy: {num_of_correct / total * 100}')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "\n",
        "# Evaluation:\n",
        "# Accuracy: 50.0\n",
        "# Precision: 0.50\n",
        "# Recall: 1.00\n",
        "# F1 Score: 0.67\n",
        "\n",
        "\n"
      ]
    }
  ]
}